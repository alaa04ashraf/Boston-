---
title: "Boston"
author: "Alaa Aboelkhair"
date: "2024-10-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(MASS)
library(ISLR2)
library(dplyr)
library(corrplot)
library(caret)
library(class)  
library(e1071)  

# mutates a dummy to indicate crime level 1 if bigger than median and 0 otherwise  
crime_med <- median(Boston$crim)
crime_level <- ifelse(Boston$crim > crime_med, 1, 0)

Boston_1 <- Boston %>% 
  mutate(crime_level)

# correlation and  scatterplot 
cor(Boston_1)

## crim zn indus chas nox
## crim 1.00000000 -0.20046922 0.40658341 -0.055891582 0.42097171
## zn -0.20046922 1.00000000 -0.53382819 -0.042696719 -0.51660371
## indus 0.40658341 -0.53382819 1.00000000 0.062938027 0.76365145
## chas -0.05589158 -0.04269672 0.06293803 1.000000000 0.09120281
## nox 0.42097171 -0.51660371 0.76365145 0.091202807 1.00000000
## rm -0.21924670 0.31199059 -0.39167585 0.091251225 -0.30218819
## age 0.35273425 -0.56953734 0.64477851 0.086517774 0.73147010
## dis -0.37967009 0.66440822 -0.70802699 -0.099175780 -0.76923011
## rad 0.62550515 -0.31194783 0.59512927 -0.007368241 0.61144056
## tax 0.58276431 -0.31456332 0.72076018 -0.035586518 0.66802320
## ptratio 0.28994558 -0.39167855 0.38324756 -0.121515174 0.18893268
## lstat 0.45562148 -0.41299457 0.60379972 -0.053929298 0.59087892
## medv -0.38830461 0.36044534 -0.48372516 0.175260177 -0.42732077
## crime_level 0.40939545 -0.43615103 0.60326017 0.070096774 0.72323480
## rm age dis rad tax
## crim -0.21924670 0.35273425 -0.37967009 0.625505145 0.58276431
## zn 0.31199059 -0.56953734 0.66440822 -0.311947826 -0.31456332
## indus -0.39167585 0.64477851 -0.70802699 0.595129275 0.72076018
## chas 0.09125123 0.08651777 -0.09917578 -0.007368241 -0.03558652
## nox -0.30218819 0.73147010 -0.76923011 0.611440563 0.66802320
## rm 1.00000000 -0.24026493 0.20524621 -0.209846668 -0.29204783
## age -0.24026493 1.00000000 -0.74788054 0.456022452 0.50645559
## dis 0.20524621 -0.74788054 1.00000000 -0.494587930 -0.53443158
## rad -0.20984667 0.45602245 -0.49458793 1.000000000 0.91022819
## tax -0.29204783 0.50645559 -0.53443158 0.910228189 1.00000000
## ptratio -0.35550149 0.26151501 -0.23247054 0.464741179 0.46085304
## lstat -0.61380827 0.60233853 -0.49699583 0.488676335 0.54399341
## medv 0.69535995 -0.37695457 0.24992873 -0.381626231 -0.46853593
## crime_level -0.15637178 0.61393992 -0.61634164 0.619786249 0.60874128
## ptratio lstat medv crime_level
## crim 0.2899456 0.4556215 -0.3883046 0.40939545
## zn -0.3916785 -0.4129946 0.3604453 -0.43615103
## indus 0.3832476 0.6037997 -0.4837252 0.60326017
## chas -0.1215152 -0.0539293 0.1752602 0.07009677
## nox 0.1889327 0.5908789 -0.4273208 0.72323480
## rm -0.3555015 -0.6138083 0.6953599 -0.15637178
## age 0.2615150 0.6023385 -0.3769546 0.61393992
## dis -0.2324705 -0.4969958 0.2499287 -0.61634164
## rad 0.4647412 0.4886763 -0.3816262 0.61978625
## tax 0.4608530 0.5439934 -0.4685359 0.60874128
## ptratio 1.0000000 0.3740443 -0.5077867 0.25356836
## lstat 0.3740443 1.0000000 -0.7376627 0.45326273
## medv -0.5077867 -0.7376627 1.0000000 -0.26301673
## crime_level 0.2535684 0.4532627 -0.2630167 1.00000000


pairs(Boston_1)

# splitting the data to training and testing 70% train and 30% test 
set.seed(1)
Boston_1$crime_level <- as.factor(Boston_1$crime_level)
train_index <- createDataPartition(Boston_1$crime_level, p = 0.7, list = FALSE)
train_data <- Boston_1[train_index, ]
test_data <- Boston_1[-train_index, ]

# training and testing subsets with different predictors(hint)
subset_1 <- train_data[, c("lstat","age", "rm", "crime_level")]
subset_2 <- train_data[, c("lstat","age",  "rm", "zn", "crime_level" )]
subset_3 <- train_data[, c("lstat","age",  "rm", "zn","nox","dis", "crime_level" )]
subset_4 <- train_data[, c("lstat","age","indus", "rm", "zn","nox","dis","rad", 
                           "tax", "ptratio", "medv", "crime_level")]

subset_1_test <- test_data[, c("lstat","age", "rm", "crime_level")]
subset_2_test <- test_data[, c("lstat","age",  "rm", "zn", "crime_level" )]
subset_3_test <- test_data[, c("lstat","age",  "rm", "zn","nox","dis", "crime_level" )]
subset_4_test <- test_data[, c("lstat","age","indus", "rm", "zn","nox","dis","rad", 
                               "tax", "ptratio", "medv", "crime_level")]

# to store all the results (chatgpt helped me in this)
results <- data.frame(Model = character(),
                      Subset = integer(),
                      Accuracy = numeric(),
                      Error = numeric(),
                      stringsAsFactors = FALSE)

# function to calculate accuracy and error (chatgpt helped me in this to get a nicer version)
calc_accuracy_error <- function(predictions, actual) {
  cm <- confusionMatrix(predictions, actual)
  accuracy <- cm$overall['Accuracy']
  error <- 1 - accuracy
  return(c(accuracy, error))
}

# logistic regression on all subsets:
glm_models <- list(glm_1 = glm(crime_level ~ lstat + age + rm, data = subset_1, family = "binomial"),
                   glm_2 = glm(crime_level ~ lstat + age + rm + zn, data = subset_2, family = "binomial"),
                   glm_3 = glm(crime_level ~ lstat + age + rm + zn + nox + dis, data = subset_3, family = "binomial"),
                   glm_4 = glm(crime_level ~ lstat + age + rm + indus + zn + nox + dis + rad + tax + ptratio + medv, 
                               data = subset_4, family = "binomial"))

test_subsets <- list(subset_1_test, subset_2_test, subset_3_test, subset_4_test)

for (i in 1:4) {
  glm_pred <- predict(glm_models[[i]], test_subsets[[i]], type = "response")
  glm_class <- ifelse(glm_pred > 0.5, 1, 0) # probability 
  acc_err <- calc_accuracy_error(factor(glm_class), factor(test_subsets[[i]]$crime_level)) # I used the function here to show accuracy
  
  # here it binds result from logistic regression to the table i created. same logic for the other models  
  results <- rbind(results, data.frame(Model = "Logistic Regression", Subset = i, 
                                       Accuracy = acc_err[1], Error = acc_err[2]))
}

# LDA on all subsets

lda_models <- list(lda_1 = lda(crime_level ~ lstat + age + rm, data = subset_1),
                   lda_2 = lda(crime_level ~ lstat + age + rm + zn, data = subset_2),
                   lda_3 = lda(crime_level ~ lstat + age + rm + zn + nox + dis, data = subset_3),
                   lda_4 = lda(crime_level ~ lstat + age + rm + indus + zn + nox + dis + rad + tax + ptratio + medv, 
                               data = subset_4))

for (i in 1:4) {
  lda_pred <- predict(lda_models[[i]], test_subsets[[i]])$class
  acc_err <- calc_accuracy_error(factor(lda_pred), factor(test_subsets[[i]]$crime_level))
  
  results <- rbind(results, data.frame(Model = "LDA", Subset = i, 
                                       Accuracy = acc_err[1], Error = acc_err[2]))
}

# Naive Bayes on all subsets
nb_models <- list(nb_1 = naiveBayes(crime_level ~ lstat + age + rm, data = subset_1),
                  nb_2 = naiveBayes(crime_level ~ lstat + age + rm + zn, data = subset_2),
                  nb_3 = naiveBayes(crime_level ~ lstat + age + rm + zn + nox + dis, data = subset_3),
                  nb_4 = naiveBayes(crime_level ~ lstat + age + rm + indus + zn + nox + dis + rad + tax + ptratio + medv, 
                                    data = subset_4))

for (i in 1:4) {
  nb_pred <- predict(nb_models[[i]], test_subsets[[i]])
  acc_err <- calc_accuracy_error(factor(nb_pred), factor(test_subsets[[i]]$crime_level))
  
  results <- rbind(results, data.frame(Model = "Naive Bayes", Subset = i, 
                                       Accuracy = acc_err[1], Error = acc_err[2]))
}

# KNN on all subsets using loops
k_value <- 5  
train_subsets <- list(subset_1, subset_2, subset_3, subset_4)

for (i in 1:4) {
  train_data <- train_subsets[[i]]
  test_data <- test_subsets[[i]]
  
  train_predictors <- train_data[, -ncol(train_data)]
  train_labels <- train_data$crime_level
  
  test_predictors <- test_data[, -ncol(test_data)]
  test_labels <- test_data$crime_level
  
  knn_pred <- knn(train = train_predictors, test = test_predictors, cl = train_labels, k = k_value)
  
  acc_err <- calc_accuracy_error(factor(knn_pred), factor(test_labels))
  
  results <- rbind(results, data.frame(Model = "KNN", Subset = i, 
                                       Accuracy = acc_err[1], Error = acc_err[2]))
}

# The table that have all the results 
print(results)

## Model Subset Accuracy Error
## Accuracy Logistic Regression 1 0.7600000 0.24000000
## Accuracy1 Logistic Regression 2 0.7800000 0.22000000
## Accuracy2 Logistic Regression 3 0.8533333 0.14666667
## Accuracy3 Logistic Regression 4 0.9066667 0.09333333
## Accuracy4 LDA 1 0.7666667 0.23333333
## Accuracy5 LDA 2 0.7800000 0.22000000
## Accuracy6 LDA 3 0.8200000 0.18000000
## Accuracy7 LDA 4 0.8466667 0.15333333
## Accuracy8 Naive Bayes 1 0.7666667 0.23333333
## Accuracy9 Naive Bayes 2 0.7600000 0.24000000
## Accuracy10 Naive Bayes 3 0.8200000 0.18000000
## Accuracy11 Naive Bayes 4 0.8066667 0.19333333
## Accuracy12 KNN 1 0.7266667 0.27333333
## Accuracy13 KNN 2 0.7600000 0.24000000
## Accuracy14 KNN 3 0.7733333 0.22666667
## Accuracy15 KNN 4 0.8933333 0.1066666
```
